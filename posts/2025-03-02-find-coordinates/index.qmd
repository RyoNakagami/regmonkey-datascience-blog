---
title: "æ•µã®ç ²å°ã®åº§æ¨™ã‚’æ¢ã› 1/N"
author: "Ryo Nakagami"
date: "2025-03-02"
date-modified: last-modified
categories: [å¹¾ä½•]
listing_category: datascience-statistics-series
comments:
  utterances:
       repo: RyoNakagami/regmonkey-datascience-blog
       label: discussion
       issue-term: title
---

## å††ã®å¼

$(a, b)$ ã‚’ä¸­å¿ƒç‚¹ã¨ã™ã‚‹åŠå¾„ $r$ ã®å††ã‚’plotã™ã‚‹ã“ã¨ã‚’è€ƒãˆã¾ã™ï¼

åŠå¾„ $r$ ã®å††ã®ä¸­å¿ƒç‚¹ã®åº§æ¨™ãŒåŸç‚¹ $(0, 0)$ ã«ã‚ã‚‹å ´åˆï¼Œå††å‘¨ä¸Šã®ç‚¹ $P = (x, y)$ ã¯ï¼Œ$P$ ã‹ã‚‰ $x$ è»¸ã«ä¸‹ã‚ã—ãŸå‚ç·šã¨ $x$ è»¸ãŒäº¤ã‚ã‚‹ç‚¹ã‚’ $Q$ ã¨ã—ãŸã¨ã
$\triangle OPQ$ ã¯æ–œè¾º $r$ï¼Œé«˜ã• $y$, åº•è¾ºã®é•·ã• $x$ ã¨ãªã‚‹ç›´è§’ä¸‰è§’å½¢ã‚’æ§‹æˆã™ã‚‹ã®ã§ï¼Œä¸‰å¹³æ–¹ã®å®šç†ã‚ˆã‚Š

$$
r^2 = x^2 + y^2
$$

ã“ã‚ŒãŒå††ä¸Šã®åº§æ¨™ãŒæº€ãŸã™æ–¹ç¨‹å¼ã¨ãªã‚Šã¾ã™ï¼åŸç‚¹ã‚’ä¸­å¿ƒç‚¹ã¨ã™ã‚‹å ´åˆã‚’è€ƒãˆã¾ã—ãŸãŒï¼Œä¸­å¿ƒãŒ $(a, b)$ï¼ŒåŠå¾„ $r$ ã®å††ã®å¼ã¯åŒæ§˜ã®æ–¹æ³•ã§

$$
r^2 = (x - a)^2 + (y - b)^2 \label{#eq-circle}
$$

ã¨è¡¨ã™ã“ã¨ãŒå‡ºæ¥ã¾ã™ï¼

```{python}
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.tri as mtri

# set params
R = 1
O = (2, 1)

# variables
theta = np.linspace(0, 2 * np.pi, 1000)
x = np.cos(theta) * R + O[0]
y = np.sin(theta) * R + O[1]

A = (x[100], y[100])
triangles = [[0, 1, 2]]
x_trinagle = [O[0], A[0], A[0]]
y_trinagle = [O[1], O[1], A[1]]
triang = mtri.Triangulation(x_trinagle, y_trinagle, triangles)

# plot
fig, ax = plt.subplots(figsize=(8, 6))
ax.grid()
ax.set_aspect("equal")
ax.set_xlim(0.5, 3.5)
ax.set_ylim(-0.5, 2.5)


ax.plot(x, y)
ax.scatter(*O, s=8, color="k")
ax.scatter(*A, s=8, color="k")
ax.text(*O, s="O",  ha='center', va='bottom')
ax.text(*A, s="A = ($x_1, y_1$)", ha='left', va='bottom')
ax.text(2.4, 1.35, s="r")
ax.text(2.4, 0.9, s="$x_1 - 2$", ha='center', va='bottom')
ax.text(A[0]+0.05, 1.35, s="$y_1 - 1$", ha='left', va='bottom')

# add lines
ax.triplot(triang, 'ko-')


plt.show()
```

### ï¼“ç‚¹ã‚’é€šã‚‹å††ã®æ–¹ç¨‹å¼

```{python}
# set params
R = 2.5
O = (3, 1/2)

# variables
theta = np.linspace(0, 2 * np.pi, 1000)
x = np.cos(theta) * R + O[0]
y = np.sin(theta) * R + O[1]

P = (1, -1)
Q = (3, 3)
R = (1, 2)

# plot
fig, ax = plt.subplots(figsize=(8, 6))
ax.grid()
ax.set_aspect("equal")

ax.plot(x, y)
ax.scatter(*O, s=8, color="k")
ax.scatter(*P, s=8, color="k")
ax.scatter(*Q, s=8, color="k")
ax.scatter(*R, s=8, color="k")
ax.text(*O, s="O",  ha='center', va='bottom')
ax.text(*P, s="P",  ha='center', va='bottom')
ax.text(*Q, s="Q", ha='left', va='bottom')
ax.text(*R, s="R", ha='left', va='bottom')


plt.show()

```


ï¼“ç‚¹ $P = (1, -1), Q = (3, 3), R = (1, 2)$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ã—ã¦ï¼Œã“ã®ï¼“ç‚¹ã‚’é€šã‚‹å††ã‚’æ±‚ã‚ã‚‹å•é¡Œã‚’è€ƒãˆã¾ã™ï¼

\eqref{#eq-circle} ã‚’å±•é–‹ã™ã‚‹ã¨

$$
x^2 + y^2 - 2ax - 2by + a^2 + b^2 = r^2
$$

ã“ã‚Œã‚’æ•´ç†ã—ã¦

$$
x^2 + y^2 + Ax + By + C = 0 \label{#eq-basemodel}
$$

ã¨å¤‰å½¢ã—ã¾ã™ï¼ã“ã“ã§ï¼Œ$P = (1, -1), Q = (3, 3), R = (1, 2)$ ã®æƒ…å ±ã‚’ç”¨ã„ã‚‹ã¨

$$
\begin{gather}
2 + A - B + C = 0\\
18 + 3A + 3B + C = 0\\
5 + A +2B + C = 0
\end{gather}
$$

ã¨ã„ã† $A,B,C$ ã«ã¤ã„ã¦ã®é€£ç«‹æ–¹ç¨‹å¼ã‚’å¾—ã‚‹ã“ã¨ãŒå‡ºæ¥ã¾ã™ï¼ã“ã‚Œã‚’è§£ãã¨

$$
A = -6, B = -1, C = 3
$$

å¾“ã£ã¦ï¼Œ

$$
(x - 3)^2 + (y - 0.5)^2 = 2.5^2 \label{#eq-ans1}
$$

#### å¤–æ¥å††ã‹ã‚‰æ±‚ã‚ã‚‹

ç‚¹ $P, Q, R$ ã‹ã‚‰ãªã‚‹ä¸‰è§’å½¢ã®å¤–æ¥å††ã¨ã—ã¦æ±‚ã‚ãŸã„å††ã‚’æ‰ãˆã‚‹ã“ã¨ã‚‚å‡ºæ¥ã¾ã™ï¼
å¤–æ¥å††ã®å††å¿ƒã¯ä¸‰è§’å½¢ã®å„ç·šåˆ†ã®å‚ç›´äºŒç­‰åˆ†ç·šã®äº¤ç‚¹ã¨ã—ã¦æ±‚ã‚ã‚‹ã“ã¨ãŒå‡ºæ¥ã¾ã™ï¼

```{python}
def func_pq(x):
    a = - (P[0] - Q[0]) / (P[1] - Q[1])
    b = - a * (P[0] + Q[0]) / 2 + (P[1] + Q[1]) / 2

    return a*x + b

def func_pr(x):
    a = - (P[0] - R[0]) / (P[1] - R[1])
    b = - a * (P[0] + R[0]) / 2 + (P[1] + R[1]) / 2

    return a*x + b

x_lin = np.array([0, 6])
triangles = [[0, 1, 2]]
x_trinagle = [P[0], Q[0], R[0]]
y_trinagle = [P[1], Q[1], R[1]]
triang = mtri.Triangulation(x_trinagle, y_trinagle, triangles)



# plot
fig, ax = plt.subplots(figsize=(8, 6))
ax.grid()
ax.set_aspect("equal")
ax.set_xlim(0, 6)
ax.set_ylim(-2.5, 3.5)
ax.plot(x_lin, func_pq(x_lin), label='PQ Perpendicular bisector', linestyle='--', color='gray')
ax.plot(x_lin, func_pr(x_lin), label='PR Perpendicular bisector', linestyle=':', color='gray')


ax.plot(x, y)
ax.scatter(*O, s=8, color="k")
ax.scatter(*P, s=8, color="k")
ax.scatter(*Q, s=8, color="k")
ax.scatter(*R, s=8, color="k")
ax.text(*O, s="O",  ha='center', va='bottom')
ax.text(*P, s="P",  ha='center', va='bottom')
ax.text(*Q, s="Q", ha='left', va='bottom')
ax.text(*R, s="R", ha='left', va='bottom')

# add lines
ax.triplot(triang, 'ko-')

plt.legend(loc='center left', bbox_to_anchor=(1, 0.95))
plt.show()

```


$PQ$ ã®å‚ç›´äºŒç­‰åˆ†ç·š $f(x)$ ã¯

$$
\begin{align}
f(x) 
    &= -\frac{P_x - Q_x}{P_y - Q_y}x + \frac{P_y + Q_y}{2} + \frac{P_x - Q_x}{P_y - Q_y} \frac{P_x + Q_y}{2}\\
    &= -\frac{P_x - Q_x}{P_y - Q_y}x + \frac{P_x^2 - Q_x^2 + P_y^2 - Q_y^2}{2(P_y - Q_y)}
\end{align}
$$

åŒæ§˜ã« $PR$ ã®å‚ç›´äºŒç­‰åˆ†ç·š $g(x)$ ã¯

$$
\begin{align}
g(x) 
    &= -\frac{P_x - R_x}{P_y - R_y}x + \frac{P_y + R_y}{2} + \frac{P_x - R_x}{P_y - R_y} \frac{P_x + R_y}{2}\\
    &= -\frac{P_x - R_x}{P_y - R_y}x + \frac{P_x^2 - R_x^2 + P_y^2 - R_y^2}{2(P_y - R_y)}
\end{align}
$$

ã“ã“ã‹ã‚‰ $f(x), g(x)$ ãŒäº¤å·®ã™ã‚‹ç‚¹ã‚’æ±‚ã‚ã‚‹ã“ã¨ã§å¤–æ¥å††ã®å††å¿ƒã‚’æ±‚ã‚ã‚‹ã“ã¨ãŒå‡ºæ¥ã¾ã™ï¼

å°‘ã—ã‚ã‚“ã©ãã•ã„ã®ã§ï¼Œæ•°å€¤è¨ˆç®—ã§èª¬ã„ã¦ã¿ã‚‹ã¨

```{python}
#| code-fold: show
a = - (P[0] - Q[0]) / (P[1] - Q[1])
b = - a * (P[0] + Q[0]) / 2 + (P[1] + Q[1]) / 2
c = - (P[0] - R[0]) / (P[1] - R[1])
d = - c * (P[0] + R[0]) / 2 + (P[1] + R[1]) / 2

print((d-b)/(a-c), a * (d-b)/(a-c) + b)
```

\eqref{#eq-ans1} ã¨ä¸€è‡´ã™ã‚‹è¨ˆç®—çµæœã¨ãªã‚‹ã“ã¨ãŒç¢ºã‹ã‚ã‚‰ã‚Œã¾ã—ãŸï¼

<div class="blog-custom-border" style="margin-top: 1rem; margin-bottom: 1rem;">
<strong >ğŸ“˜ REMARKS</strong> <br>

ä¸Šè¨˜ã®å‚ç›´äºŒç­‰åˆ†ç·šã®äº¤ç‚¹ã‚’ $(x_0, y_0)$ ã¨ã—ãŸã¨ãï¼Œæ•´ç†ã™ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼

$$
A = \left(\begin{array}{cc}
R_y - Q_y & -(P_y - Q_y)\\
-(R_x - Q_x) & P_x - Q_x\\
\end{array}\right)
$$

ã¨ã—ãŸã¨ãï¼Œ

$$
\left(\begin{array}{c}
x_0\\
y_0
\end{array}\right)
    = \frac{1}{\operatorname{det}A} A\left(\begin{array}{c}
(P_x^2 - Q_x^2 + P_y^2 - Q_y^2)/2\\
(R_x^2 - Q_x^2 + R_y^2 - Q_y^2)/2
\end{array}\right)
$$

å®Ÿéš›ã«è¨ˆç®—ã—ã¦ã¿ã‚‹ã¨

```{python}
#| code-fold: show
A_array = np.array([[R[1] - Q[1], -(P[1] - Q[1])], [-(R[0] - Q[0]), P[0] - Q[0]]])
B_array = np.array(
    [
        [(P[0]**2 - Q[0]**2 + P[1]**2 - Q[1]**2) / 2],
        [(R[0]**2 - Q[0]**2 + R[1]**2 - Q[1]**2) / 2],
    ]
)

result = np.ravel((A_array @ B_array) / np.linalg.det(A_array))
radius = np.sqrt(np.sum((np.array(P) - result) **2))
print(f"ä¸­å¿ƒç‚¹ = ({result}), åŠå¾„ = {radius}")
```


</div>

## æ•µã®ç ²å°ã®åº§æ¨™ã‚’æ¢ã›

<div class="blog-custom-border" style="margin-top: 1rem; margin-bottom: 1rem;">
::: {#exr- .custom_problem }
<br>

ã¨ã‚ã‚‹å›ºå®šã®ï¼‘åœ°ç‚¹ã‹ã‚‰è‡ªè»é ˜åœ°ã«å¯¾ã—ã¦æ•µãŒç ²æ’ƒã‚’ã‹ã‘ã¦ãã¦ã„ã‚‹ã¨ã—ã¾ã™.
æ•µã®ç ²å°ã¯è§’åº¦ã®ã¿ã‚’èª¿æ•´ã§ãã‚‹ã ã‘ã§ï¼Œç ²æ’ƒäºˆå®šè·é›¢ $r$ ã¯ä¸€å®šã¨ã—ã¾ã™ï¼ãŸã ã—ï¼Œå®Ÿéš›ã®ç ²æ’ƒè·é›¢ã¯é¢¨å‘ãªã©ã®å¤–ä¹±è¦å› ã«ã‚ˆã£ã¦ãƒã‚¤ã‚ºãŒæ··ã˜ã£ã¦ã„ã‚‹ã¨ã—ã¾ã™ï¼

ã¨ã‚ã‚‹æ—¥ã«æ•µã‹ã‚‰50å›ã®æ”»æ’ƒã‚’å—ã‘ãŸã¨ãï¼Œãã®ç ²å°åº§æ¨™ã‚’æ¨å®šã—ã¦ãã ã•ã„ï¼ç ²æ’ƒã®ãƒã‚¤ã‚ºã¯$\operatorname{i.i.d}$ã¨ã™ã‚‹ï¼

:::

</div>

\eqref{#eq-basemodel} ã‚ˆã‚Š

$$
\begin{align}
z_i = - x_i^2 - y_i^2
\end{align}
$$

ã¨å®šç¾©ã™ã‚‹ã¨ï¼Œ$e_i$ ã‚’residualã¨ã—ã¦

$$
z_i = \beta_0 + \beta_1 x_i + \beta_2 y_i + e_i
$$

ã«ã¤ã„ã¦ $(\beta_0, \beta_1, \beta_2)$ ã‚’Linear modelã§æ¨å®šã—ï¼Œãã®æ¨å®šå€¤ã‚’ $(\hat\beta_0, \hat\beta_1, \hat\beta_2)$ ã¨è¡¨ã›ã°
æ•µã®ç ²å°ã®æ¨å®šåº§æ¨™ $(\hat x, \hat y)$ åŠã³æ¨å®šè·é›¢ $\hat r$ ã¯

$$
\begin{align}
\hat x &= -\hat\beta_1/2\\
\hat y &= -\hat\beta_2/2\\
\hat r &= \sqrt{\hat x^2 + \hat y^2 - \hat\beta_0}
\end{align}
$$

ã¨è¨ˆç®—ã§ããã†ã«æ€ãˆã¾ã™ï¼

<strong > &#9654;&nbsp; Data Generating Process</strong>

- æ•µã®ç ²å°ã®åº§æ¨™ã¯ $(0, 0)$
- æ•µã¯ $(0, 0)$ ã®åœ°ç‚¹ã‹ã‚‰ç ²æ’ƒè·é›¢ $20$ ã§æ”»æ’ƒã—ã¦ãã‚‹
- å®Ÿéš›ã®ç ²æ’ƒè·é›¢ $r \sim N(20, 1)$
- ç ²æ’ƒè§’åº¦ã¯ $\left[\displaystyle{\frac{\pi}{3}, \frac{\pi}{2}}\right]$ ã®ç¯„å›²ã§ä¸€æ§˜åˆ†å¸ƒã§å®šã¾ã‚‹

ã¨ã„ã†Data Generating Processã¨ã—ã¾ã™ï¼


```{python}
#| code-fold: show
def gdp(x_0: float, y_0: float, noise: float = 1.0, radius: float = 20,  attack_num: int = 50):
    # params
    theta = np.random.uniform(np.pi/3 , np.pi/2 , attack_num)
    r = radius + np.random.normal(0, noise, attack_num)

    x = np.cos(theta) * r + x_0
    y = np.sin(theta) * r + y_0

    return x, y
```

ã“ã®GDPã«å¾“ã†å½¢ã§æ”»æ’ƒã•ã‚ŒãŸã¨ã™ã‚‹ã¨ãï¼Œãã®æ•£å¸ƒå›³ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼

```{python}
np.random.seed(42)

x_attack, y_attack = gdp(0, 0)

fig, ax = plt.subplots()
ax.scatter(x_attack, y_attack)
ax.set_xlim(-1, np.max([np.max(x_attack), np.max(y_attack)]) + 1)
ax.set_ylim(-1, np.max([np.max(x_attack), np.max(y_attack)]) + 1)
ax.grid()
ax.set_aspect('equal')
plt.show()
```

<strong > &#9654;&nbsp; OLS Monte Carlo Simulation</strong>

OLSã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®šå€¤ã‚’ $1,000$ å›simulationã—ï¼Œãã®çµ„ã¿åˆã‚ã›ã‚’kde plotã—ãŸã‚‚ã®ãŒä»¥ä¸‹ã¨ãªã‚Šã¾ã™ï¼xåº§æ¨™ã«ã¤ã„ã¦ãƒã‚¤ã‚¢ã‚¹ãŒã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼
ç ²æ’ƒè·é›¢ã«é–¢ã—ã¦ã‚‚ä¸è‡ªç„¶ãªæ¨å®šå€¤ã¨ãªã£ã¦ã„ã¾ã™ï¼

```{python}
import pandas as pd
import statsmodels.api as sm
import seaborn as sns

def gpd_dataframe(x: float = 0, y: float = 0, noise: float = 1.0):
    x_attack, y_attack = gdp(x, y, noise)
    df = pd.DataFrame(
        {
            "x_coordinate": x_attack,
            "y_coordinate": y_attack,
        }
    )

    return df


def ols_solver(
    df: pd.DataFrame, xy_columns: tuple[str] = ("x_coordinate", "y_coordinate")
):
    Y = -(df[xy_columns[0]] ** 2) - df[xy_columns[1]]
    X = sm.add_constant(df.loc[:, xy_columns])

    # regression
    model = sm.OLS(Y, X)
    results = model.fit()

    # convert estimates to target params
    x_hat = -results.params[xy_columns[0]] / 2
    y_hat = -results.params[xy_columns[1]] / 2
    r_hat_sqr = (x_hat**2 + y_hat**2 - results.params['const'])
    r_hat = np.sqrt(r_hat_sqr) if r_hat_sqr > 0 else np.nan

    return [x_hat, y_hat, r_hat]


def estimator_simulator(func, noise: float = 1.0, iter: int = 1000):
    res = list(map(lambda x: func(gpd_dataframe(0, 0, noise)), range(iter)))
    return np.array(res)

fig, ax = plt.subplots(1, 2)

ols_res = estimator_simulator(ols_solver)
sns.kdeplot(x=ols_res[:, 0], y=ols_res[:, 1], cmap="Blues", fill=True, ax=ax[0])
sns.kdeplot(x=ols_res[:, 2], cmap="Blues", fill=True, ax=ax[1])

# Show the plot
ax[0].set_xlabel("estimated x coordinate")
ax[0].set_ylabel("estimated y coordinate")
ax[0].set_aspect('equal')
ax[0].set_title("OLS 2D Density Plot\nwith 1000 iterations")
ax[1].set_title("Raidus Density Plot\nwith 1000 iterations")
plt.show()
```

ãã‚‚ãã‚‚ $r_i = 20 + \epsilon_i$ ã¨æ±ºå®šã•ã‚Œã¦ã„ã¾ã™ãŒ

$$
\beta_0 = a^2 + b^2 - (r + \epsilon_i)^2
$$

ã§æ±ºå®šã•ã‚Œã¦ãŠã‚Šï¼Œã“ã‚Œã‚’è¸ã¾ãˆã¦ OLSã®ãƒ¢ãƒ‡ãƒ«ã‚’è¦‹ã¦ã¿ã‚‹ã¨

$$
z_i = \left(a^2 + b^2 - r^2 - \epsilon_i^2 - 2r\epsilon \right) - 2ax_i - 2by_i
$$

ã¨ãªã‚‹ã®ã§ï¼Œãã‚‚ãã‚‚unbiasedãªæ¨å®šé‡ã«ãªã£ã¦ã„ãªã„ã¨åˆ¤æ–­ã§ãã¾ã™

<strong > &#9654;&nbsp; Regression with MLE</strong>

\eqref{#eq-circle} ã«å‰‡ã‚Šï¼Œã‚‚ã£ã¨ç›´æ¥çš„ã«

$$
L(\beta) = (\sqrt{(x_i - \beta_1)^2 + (y_i - \beta_2)^2} - \beta_0)^2
$$

ã‚’æœ€å°ã™ã‚‹å½¢ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’æ¨å®šã—ã¦ã¿ã¾ã™ï¼ã“ã®ã¨ãï¼residualãŒ$N(0, \sigma)$ ã«å¾“ã†ãªã‚‰ã°Likelihoodã¯

$$
f(X_i\vert \beta, \sigma) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{L(\beta)}{2\sigma^2}\right) 
$$

ã¨è¡¨ã›ã‚‹ã®ã§ï¼Œã“ã‚Œã‚’ç”¨ã„ã¦è§£ã„ã¦ã¿ã¾ã™ï¼


```{python}
import numpy as np
from scipy.optimize import minimize


def lik(parameters, x, y):
    x_0 = parameters[0]
    y_0 = parameters[1]
    r_0 = parameters[2]
    sigma = parameters[3]
    g_x = (np.sqrt((np.sqrt((x - x_0) ** 2 + (y - y_0) ** 2) - r_0)**2)) ** 2

    L = len(x) / 2 * np.log(sigma**2) +  1 / (2 * sigma**2) * np.sum(g_x)
    return L


def mle_solver(
    df: pd.DataFrame, xy_columns: tuple[str] = ("x_coordinate", "y_coordinate")
):

    x_attack = df[xy_columns[0]].values
    y_attack = df[xy_columns[1]].values
    lik_model = minimize(
        lambda params: lik(params, x_attack, y_attack),
        np.array([1, 1, 20, 1]),
        method="L-BFGS-B",
    )
    return lik_model["x"]


mle_res = estimator_simulator(mle_solver)

# plot
fig, axes= plt.subplots(1, 2)

sns.kdeplot(x=mle_res[:, 0], y=mle_res[:, 1], cmap="Blues", fill=True, ax=axes[0])
sns.kdeplot(x=mle_res[:, 2], cmap="Blues", fill=True, ax=axes[1])

# Show the plot
axes[0].set_xlabel("estimated x coordinate")
axes[0].set_ylabel("estimated y coordinate")
axes[0].set_title("MLE 2D Density Plot\nwith 1000 iterations")
axes[0].set_aspect("equal")
axes[1].set_title("Raidus Density Plot\nwith 1000 iterations")

plt.show()
```

å®šå¼åŒ–ã¯æ­£ã—ã„ã¯ãšã§ã™ãŒï¼Œ$(x_0, y_0, r_0)$ ã¯åŠ¹ç‡çš„ãªæ¨å®šé‡ã¨ãªã£ã¦ã„ãªã„ç–‘ã„ãŒã‚ã‚‹ã“ã¨ãŒèª­ã¿å–ã‚Œã¾ã™ï¼

æ¬¡ã«ï¼Œ

$$
L(\beta) = \sqrt{(x_i - \beta_1)^2 + (y_i - \beta_2)^2 - \beta_0^2} \label{#eq-mle}
$$

ã‚’Loss functionã¨ã—ã¦æ¨å®šã—ã¦ã¿ã¾ã™ï¼


```{python}
import numpy as np
from scipy.optimize import minimize


def lik(parameters, x, y):
    x_0 = parameters[0]
    y_0 = parameters[1]
    r_0 = parameters[2]
    sigma = parameters[3]
    g_x = (x - x_0) ** 2 + (y - y_0) ** 2 - r_0**2

    L = len(x) / 2 * np.log(sigma**2) +  1 / (2 * sigma**2) * np.sum(g_x)
    return L


def mle_solver(
    df: pd.DataFrame, xy_columns: tuple[str] = ("x_coordinate", "y_coordinate")
):

    x_attack = df[xy_columns[0]].values
    y_attack = df[xy_columns[1]].values
    lik_model = minimize(
        lambda params: lik(params, x_attack, y_attack),
        np.array([1, 1, 20, 1]),
        method="L-BFGS-B",
    )
    return lik_model["x"]


mle_res = estimator_simulator(mle_solver)

# plot
fig, axes= plt.subplots(1, 2)

sns.kdeplot(x=mle_res[:, 0], y=mle_res[:, 1], cmap="Blues", fill=True, ax=axes[0])
sns.kdeplot(x=mle_res[:, 2], cmap="Blues", fill=True, ax=axes[1])

# Show the plot
axes[0].set_xlabel("estimated x coordinate")
axes[0].set_ylabel("estimated y coordinate")
axes[0].set_title("MLE 2D Density Plot\nwith 1000 iterations")
axes[0].set_aspect("equal")
axes[1].set_title("Raidus Density Plot\nwith 1000 iterations")

plt.show()
```



å…ˆç¨‹ã‚ˆã‚Šã¯ç²¾åº¦è‰¯ãæ¨å®šã§ãã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™ãŒï¼Œ\eqref{#eq-mle} ã¯

$$
L(\beta) = \sqrt{\epsilon_i^2 + 2\beta_0 \epsilon_i}
$$

ã¨ãªã‚‹ã®ã§ï¼Œãã‚‚ãã‚‚MLEã®å®šå¼åŒ–ãŒé–“é•ã£ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼ã¾ãŸï¼Œ$\beta_0$ ãŒå¤§ãã„ã»ã©residualãŒå¤§ãããªã‚‹å‚¾å‘ãŒã‚ã‚‹ã“ã¨ã‹ã‚‰ï¼Œunbiasedãªæ¨å®šé‡ã¯å¾—ã‚‰ã‚Œã¦ã„ãªã„ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼

<strong > &#9654;&nbsp; Regression with stan</strong>

cmdstanã‚’ç”¨ã„ã¦ç ²å°åº§æ¨™ã‚’æ¨å®šã™ã‚‹ä¾‹ã‚’ç´¹ä»‹ã—ã¾ã™ï¼ã¾ãšstan modelã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­å®šã—ã¾ã™ï¼

- ç ²æ’ƒè·é›¢ã®ãƒã‚¤ã‚ºã«ã¤ã„ã¦ $N(0, 1)$ ã§ã‚ã‚‹ã“ã¨ãŒæ—¢ã«ã‚ã‹ã£ã¦ã„ã‚‹çŠ¶æ³ã‚’æƒ³å®š
- äºˆå®šç ²æ’ƒè·é›¢ $r_0$ ã¯ $\operatorname{Uniform}(2, 30)$ ã®äº‹å‰åˆ†å¸ƒãŒã‚ã‚‹

```{.stan}
data {
    int<lower=1> N;  // Number of data points
    array[N] real y; // outcomes
    array[N] real x; // outcomes
}

parameters {
    real<lower=0> r_0; // probability of success
    real x_0;             // Center x-coordinate
    real y_0;             // Center y-coordinate
}

model {
    // priors
    r_0 ~ uniform(2, 30);
    real sigma = 1;

    // objective loss
    array[N] real circle_equation;
    for (i in 1:N) {
        circle_equation[i] = sqrt((x[i] - x_0)^2 + (y[i] - y_0)^2) - r_0;
    }

    circle_equation ~ normal(0, sigma);
}
```

ãã®å¾Œï¼Œã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦æ¨å®šã—ãŸã‚‚ã®ãŒä»¥ä¸‹ã¨ãªã‚Šã¾ã™ï¼

```{python}
# | code-fold: false
from cmdstanpy import CmdStanModel

df_stan = gpd_dataframe(0, 0, 1)
data = {
    "N": df_stan.shape[0],
    "y": df_stan.y_coordinate.values,
    "x": df_stan.x_coordinate.values,
    "sigma": 1,
}

model = CmdStanModel(stan_file="./stanmodel.stan")
fit = model.sample(data=data, seed=42)
fit.summary()
```

Credible intervalã‚’è¦‹ã‚‹ã¨ $(0, 0)$ ã¯æ¨å®šåŒºé–“ã«å«ã¾ã‚Œã¦ã„ã‚‹ä¸€æ–¹ï¼ŒMean, Medianã¨ã‚‚ã« $y_0$ ã®æ–¹ã¯ä¹–é›¢ã—ãŸå€¤ãŒæ¨å®šã•ã‚Œã¦ã—ã¾ã£ã¦ã„ã¾ã™ï¼
